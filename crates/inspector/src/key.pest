prefix =  { "lora_unet" | "lora_te_text_model_encoder" | "lora_te" ~ num ~ "_text_model_encoder" }
num    = _{ ASCII_DIGIT+ }
sep    = _{ "_" }

block_id          = { num }
transformer_block = { "transformer_blocks" ~ sep ~ block_id }
time_embedding    = { "time_embedding" ~ sep ~ "linear" ~ sep ~ block_id }
layers            = { "layers" ~ sep ~ block_id }
down_block        = { ("down_blocks") ~ sep ~ block_id }
input_block       = { "input_blocks" ~ sep ~ block_id ~ sep ~ block_id }
up_block          = { "up_blocks" ~ sep ~ block_id }
output_block      = { "output_blocks" ~ sep ~ block_id ~ sep ~ block_id }
mid_block         = { "mid_block" }
middle_block      = { "middle_block" ~ sep ~ block_id }
block             = { (layers | down_block | input_block | mid_block | middle_block | up_block | output_block) }

block_type_id = { num }
attentions    = { "attentions" ~ sep ~ block_type_id }
resnets       = { "resnets" ~ sep ~ block_type_id }
upsamplers    = { "upsamplers" ~ sep ~ block_type_id }
downsamplers  = { "downsamplers" ~ sep ~ block_type_id }
block_type    = { (resnets | attentions | upsamplers | downsamplers) }

out_id        =  { num }
attn_id       =  { num }
sub_sep       = _{ "." }
attn          =  { "attn" ~ attn_id }
self_attn     =  { "self_attn" ~ sep ~ ("k_proj" | "out_proj" | "q_proj" | "v_proj") }
mlp           =  { "mlp" }
fc            =  { "fc" ~ out_id }
to            =  { "to" ~ sep ~ ("v" | "k" | "q" | "out" ~ sep ~ out_id) }
conv          =  { "conv" ~ num | "conv_shortcut" | "conv_in" | "conv_out" | "conv" }
proj          =  { "proj_in" | "proj_out" }
time_emb_proj =  { "time_emb_proj" }

// Hada
layer_id        = { num }
in_layer        = { "in_layers" ~ sep ~ layer_id }
out_layer       = { "out_layers" ~ sep ~ layer_id }
emb_layer       = { "emb_layers" ~ sep ~ layer_id }
op              = { "op" }
skip_connection = { "skip_connection" }

ff_net     = { "ff_net" ~ sep ~ num ~ (sep ~ "proj")? }
hada_block = { in_layer | emb_layer | out_layer | skip_connection | op }
sub_block  = { (self_attn | hada_block | mlp ~ sep ~ fc | (transformer_block ~ sep ~ (attn ~ sep ~ to | ff_net)) | proj | conv | time_emb_proj) }

lora_up     = { "lora_up" }
lora_down   = { "lora_down" }
lora_weight = { (lora_up | lora_down) ~ sub_sep ~ "weight" }
lora_alpha  = { "alpha" }

hada_a      = { "a" }
hada_b      = { "b" }
hada_weight = { "hada_w" ~ num ~ sep ~ (hada_a | hada_b) }

oft_post  = { "oft_diag" }
lokr_post = { "lokr_w" ~ num }
hada_post = { hada_weight }
lora_post = { (lora_weight | lora_alpha) }

post = { sub_sep ~ (oft_post | lora_post | hada_post | lokr_post) }

key = { SOI ~ prefix ~ sep ~ ((block ~ sep ~ (block_type ~ sep)? ~ sub_block) | conv | time_embedding) ~ post? ~ EOI }
